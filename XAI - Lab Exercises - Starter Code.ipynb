{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Lab 1 - Intrinsic Explainable Models\n",
    "\n",
    "#### Topics:\n",
    "- Familiarize yourself with the libraries\n",
    "- Load the Data\n",
    "- Perform Basic Exploratory Data Analysis\n",
    "- Using Intrinsic Explainable Models\n",
    "- Intrinsic Model Experiments"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "BEty5bNFiG6l"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1) Familiarize yourself with the libraries\n",
    "a) Read the description of the libraries that are used and what is the purpose of each in the"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "aAoDg4VQiG6p"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os  # python default library to implement os tasks, such as reading and creating file paths from the disk.\n",
    "import sklearn  # library with multiple machine learning models implemented already to be used for ease\n",
    "\n",
    "import numpy as np  # A staple in python math functions\n",
    "import pandas as pd  # A staple in python data processing and data science\n",
    "import seaborn as sns  # A library with many pre-configured plots to reduce time to plot data\n",
    "\n",
    "import matplotlib.pyplot as plt  # The library which has implemented most of the plots\n",
    "\n",
    "from sklearn.model_selection import train_test_split  # A utility method made for splitting training and testing data\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression  # the models implemented by sklearn\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score  # accuracy metrics"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "NBTvicgviG6s"
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "R7jIAuv4iG6u"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### a) Add the name of the file, and it’s format."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "l_cSuCuYiG6w"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "FMT = ''  # TODO: fill in the file format\n",
    "FILE_NAME = ''  # TODO: fill in the file name of the data set\n",
    "\n",
    "PATH_TO_DIR = os.getcwd()\n",
    "PATH_TO_FILE = os.path.join(PATH_TO_DIR, f'{FILE_NAME}.{FMT}')"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "1x5mGK9aiG6y"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### b) Find the method in ”pandas” libraries to read this file format.\n",
    "[Reference to pandas io submodule](https://pandas.pydata.org/docs/reference/io.html)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "vy_AbcfPiG6z"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "read_method = ()  # TODO: replace with pd.read_ method found from the pandas library\n",
    "heart_data = read_method(PATH_TO_FILE)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "9ljlZyw0iG60"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### c) Explain the following default arguments for the read method and how they link to the data being imported:\n",
    "\n",
    "#### i) Explain the use of ‘sep’ or ‘delimiter’ argument.\n",
    "#### ii) Explain the use of ‘header’ argument\n",
    "#### iii) Explain the use of ‘decimal’ argument and its importance in financial data sets.\n",
    "\n",
    "Note: to be able to explain AI methods, it is also important to have good data analyst skills, since if the data we use can not be explained or is imported incorrectly, we are then trying to explain meaningless result"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "MLlyGu20iG62"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Answers:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "mHm6yRgWiG63"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "heart_data.head()  # Print the first five records to see the data that you have loaded"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "DjBXhQ8UiG64"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In practice this is printed to make sure the row indices are imported correctly and the columns of the data are correct.\n",
    "\n",
    "In the next cells we make sure the data is clean, although for these labs only clean data is given since this is not a learning objective of the module, but it is important to be consistent with these steps."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "tsRgfrw_iG64"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3) Perform Basic Exploratory Data Analysis\n",
    "\n",
    "For our exercise we want to explain the classification from the first 13 attributes to predict the target class, if there is a heart disease or not."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "QGOXl68yiG65"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "heart_data.info()"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "3XLt_ZbTiG66"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "heart_data.describe()"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "vINv-x4WiG66"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### a) From your theory explain which model(s) can be used for a classification task."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "jeQP8nZdiG67"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "Qws9FQ9siG68"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### b)  If for some reason you were limited to using only a few attributes, for instance choose 5 out of 13 attributes from the data set to train a model, how would you choose those attributes? (note: we can not use any advanced XAI methods yet since they require a trained model)\n",
    "\n",
    "HINT: this method’s result can be shown using a heatmap."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "JVbhQ6auiG69"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Answer:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "1Cj-Ch5CiG69"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### c) How does the answer the previous question assist the explanation of the model."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "bzFdJlspiG6-"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Answer:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "yPGwTygIiG6-"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### d) Perform the method in your answer to question 3) b).\n",
    "\n",
    "Use matplotlib.pyplot.figure() for creating a figure and appropriate method in pandas library for the stated method's calculation then use seaborn.heatmap(*arguments) with appropriate arguments to display the results of the calculation.\n",
    "\n",
    "[MatPlotLib.PyPlot.Figure](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.figure.html)\n",
    "[Pandas Library Method](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.corr.html)\n",
    "[Seaborn Heatmap](https://seaborn.pydata.org/generated/seaborn.heatmap.html)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "ZvwcW1NoiG6_"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO: create a new figure\n",
    "# TODO: Use the method from pandas to perform the method on the data\n",
    "# TODO: Use Seaborn's Heatmap method to show the results."
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "kAk-tI2biG6_"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### e) What are the top 5 most “explainable” attributes for the target class?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "IdKaJ2S_iG7A"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO: write code to find what you consider to be the top 5 explainable attributes"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "iM7bVROuiG7A"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Answer:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "yotlkwefiG7B"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Split the data into Train and Test Sets\n",
    "\n",
    "For the models, we need to make a split of the train and test data sets."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "we8NyAqriG7C"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data = heart_data.drop('target', axis=1)\n",
    "labels = heart_data.target\n",
    "data.shape, labels.shape"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "9zN-cajziG7C"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=0)\n",
    "print(X_train.shape, X_test.shape)\n",
    "X_train.head(3)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "XdlNRH2viG7C"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4) Using Intrinsic Explainable Models"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "DGA2uHAJiG7D"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### a) Train a Logistic, Tree, and kNN model on the training data.\n",
    "\n",
    "[Logistic Regression Model](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)\n",
    "[Decision Tree Model](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier)\n",
    "[kNN Model](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "l6HPSSv4iG7D"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "log_model = ()  # TODO: initialise the logistic model with default parameters.\n",
    "tree_model = ()  # TODO: initialise the decision tree with default parameters.\n",
    "knn_model = ()  # TODO: initialise the knn model with default parameters."
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "0jHO-jxUiG7E"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO: fit each of the models.\n",
    "\n",
    "log_model.\n",
    "tree_model.\n",
    "knn_model.\n",
    "\n",
    "print(\"Complete\")"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "s6A5gRqyiG7E"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Predictions and Evaluation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "J-V4H921iG7F"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "predictions_log = log_model.predict(X_test)\n",
    "predictions_tree = tree_model.predict(X_test)\n",
    "predictions_knn = knn_model.predict(X_test)\n",
    "\n",
    "log_score = f1_score(y_test, predictions_log, average='macro')\n",
    "tree_score = f1_score(y_test, predictions_tree, average='macro')\n",
    "knn_score = f1_score(y_test, predictions_knn, average='macro')\n",
    "\n",
    "print(f'{log_score=}, {tree_score=}, {knn_score=}')"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "M45KY9ZhiG7G"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### b) List the parameters that were fit in the logistic regression model and write a brief explanation on what these parameters represent in a logistic model.\n",
    "\n",
    "Hint: [Read the article](https://quantifyinghealth.com/interpret-logistic-regression-intercept/)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "oiVl42tXiG7G"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO: list the coefficients and explain how can they be interpreted."
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "I0P9P2xZiG7G"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Answer:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "f4yw4-PgiG7G"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### c) Produce a decision tree from the trained model something that a person can use to explain a decision of the model.\n",
    "\n",
    "[Reference to scikit.tree Submodule](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.tree)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "S4iKF0S8iG7H"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO: create a new figure\n",
    "# TODO: Use the method from sklearn.tree to plot the decision tree\n",
    "# TODO: show the figure"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "uPKIzmmSiG7H"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### d) Explain the importance of different parameters values in Decision Trees and kNN models."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "fw-GwfuriG7I"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Answer:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "nh21WEz7iG7J"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Intrinsic Model Experiments"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "KPmC8qtkiG7J"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### a) Use different methods of splitting for Decision Tree. Plot the different accuracies and comment on how each split is different."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "ODKcmYD2iG7J"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "lR-2siNIiG7K"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### b) Plot the accuracy scores for different values of k in the kNN model and comment on how the explanation changes with different values of k."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "WqSJYwfTiG7K"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "6loIEmRtiG7K"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Lab 2 - Model Agnostic Methods\n",
    "\n",
    "#### Topics:\n",
    "- Permutation Importance\n",
    "- KernelSHAP\n",
    "- Dependence Plot\n",
    "- Model Agnostic Methods Experiment"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "anhXRkujiG7M"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import shap  # The library with the methods for SHAP implemented\n",
    "\n",
    "from sklearn.inspection import permutation_importance  # the method which has implemented the permutation importance\n",
    "from sklearn.inspection import PartialDependenceDisplay  # the method that can be used for two variables PDP"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "YDMz3aMgiG7M"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1) Permutation Importance\n",
    "\n",
    "[Reference to scikit.inspection submodule](https://scikit-learn.org/stable/modules/partial_dependence.html#partial-dependence-plots)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "Jjz5lXTUiG7N"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### a) Perform Permutation Importance for logistic model for all the attributes and plot the results. Give a brief interpretation of your results."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "805qOAnmiG7N"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "result = ()  # TODO: perform the permutation importance on the logistic model using the train data\n",
    "perm_sorted_idx = result.importances_mean.argsort()  # arrange the results by the descending order of the mean.\n",
    "\n",
    "fig, ax1 = plt.subplots(1, 1, figsize=(12, 5))\n",
    "\n",
    "ax1.title.set_text('Permutation Importance')\n",
    "ax1.boxplot(result.importances[perm_sorted_idx].T, vert=False, labels=X_test.columns[perm_sorted_idx])\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "ZdJQADDEiG7N"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### b) Explain which model is most likely to be show a bigger difference in prediction when using permutation importance."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "LKBELwhliG7O"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Answer:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "ul041PgZiG7Q"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### c) Perform Permutation Importance for two features.\n",
    "i. Choose the most explainable features and motivate why you have chosen these two features.\n",
    "ii. Run the permutation importance for each model. Comment on the differences.\n",
    "\n",
    "[Reference to Sub Module](https://scikit-learn.org/stable/modules/generated/sklearn.inspection.PartialDependenceDisplay.html)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "vLqTqLr6iG7Q"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('Computing partial dependence plots...')\n",
    "\n",
    "for fit_model in [log_model, tree_model, knn_model]:\n",
    "    fig, ax = plt.subplots(figsize=(5, 5))  # TODO: initialise a figure\n",
    "\n",
    "    column_one = X_test.columns[]  # TODO: choose a column\n",
    "    column_two = X_test.columns[]  # TODO: choose a column\n",
    "\n",
    "    PartialDependenceDisplay.from_estimator(fit_model, X_test, [(column_one, column_two)], n_jobs=3, grid_resolution=20, ax=ax)\n",
    "    ax.set_title(f'Partial Dependence Plot - {fit_model=}')\n",
    "    plt.show()"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "K5F3kbO5iG7Q"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2) KernelSHAP\n",
    "\n",
    "##### Note: You will need to have the jupyter notebook to be trusted to use the javascript methods.\n",
    "\n",
    "[Reference to shap Library](https://shap.readthedocs.io/en/latest/index.html)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "baEP1PkOiG7Q"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### a) Use a SHAP Explainer to derive SHAP Values for the logistic regression model.\n",
    "\n",
    "[Reference to KernelSHAP](https://shap-lrjball.readthedocs.io/en/latest/generated/shap.KernelExplainer.html)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "xhMK1z1FiG7R"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# We need to artificially reduce the number of data points due to computational infeasibility\n",
    "background = X_train.sample(n=82)\n",
    "\n",
    "explainer = ()  # TODO: Use the KernelSHAP method with log_model.predict and background as the parameters.\n",
    "# background can be replaced with X_train as well but that takes much longer\n",
    "shap_values = ()  #TODO: find the shap_values from the explainer."
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "YwbeZgYAiG7S"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### b) Plot a SHAP summary plot using the bar chart using all the features in the data and explain the results of the plots."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "SYDU2SPXiG7S"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO: plot the summary plot as a bar chart"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "ATr5aO2riG7T"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### c) Plot the partial dependence plot using the SHAP values for the attribute 'exang' and explain how to interpret the plot."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "cfCPFzJmiG7T"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO: plot the dependence plot"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "fy3wltAfiG7T"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### d) Plot a 'force plot' (part of shap library) for a random X_test data point, and explain the figure."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "i6JtlyeQiG7U"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "shap.initjs()  # print the JS visualization code to the notebook\n",
    "\n",
    "# TODO: plot the force_plot"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "A04nV7NViG7U"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Explanation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "Y5i1xMD6iG7V"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "explanation_contribution = pd.DataFrame(shap_values, columns=X_test.columns)\n",
    "explanation_contribution.describe()"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "J0fY_qcpiG7V"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This returns a matrix of SHAP values (# samples, # features). Each row sums to the difference between the model output for that sample and the expected value of the model output (which is stored as expected_value attribute of the explainer)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "0mgJCQMviG7W"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### e) Plot the summary plot for each of the attributes and give an interpretation of the plot."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "_NwdZNPWiG7W"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO: plot the summary plot but not as a bar chart."
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "pCj3m_G3iG7W"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3) Dependence Plot"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "SCz9aR1QiG7X"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### a) Plot a dependence plot to show the effect of ‘chol’ across the whole dataset."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "J6DFNs4NiG7X"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "shap.initjs()  # initialise the js for the dependence_plot method\n",
    "# TODO: plot the dependence plot"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "aSPQPDIjiG7Y"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Explanation:\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "XUdHIdOriG7Y"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### b) Plot the two-way PDP showing interactions between features ‘Resting blood pressure’ and ‘Chest pain type’ and explain their effect."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "lWHDVFqsiG7Z"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# TODO: initialise the js for shap\n",
    "# TODO: plot the dependence plot but this time for two features"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    },
    "id": "hMRGNJHuiG7Z"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Explanation:\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "FHMsTvVkiG7a"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4) Perform the same analysis as above for a model of your choice (other than the Logistic Model) and comment on the sensitivity of the two models by comparing and contrasting the results."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "Yqz9LYk8iG7a"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "oKtmsYw-iG7a"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Lab 3 - Explaining Deep Learning Models\n",
    "\n",
    "#### Topics:\n",
    "- Occlusion\n",
    "- Unsupervised Explainers\n",
    "- Gradient Based Explainers\n",
    "- DeepSHAP (DeepLift)\n",
    "- Attacks and Defence using Adversarial Features"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "rdCzj3U0iG7a"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.manual_seed(0)\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "from IPython.display import display\n",
    "import PIL\n",
    "import ast\n",
    "import cv2\n",
    "\n",
    "from utils import preprocess_image"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "xdpiUkJhiG7b"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "resnet_model = models.resnet18(pretrained=True)\n",
    "resnet_model.eval()"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "eGxPVXFyiG7c"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "IMAGE_NET_CLASSES_FILE = 'imagenet_classes'\n",
    "IMAGE_NET_CLASSES_FILE_FMT = 'txt'\n",
    "\n",
    "with open(os.path.join(os.getcwd(), f'{IMAGE_NET_CLASSES_FILE}.{IMAGE_NET_CLASSES_FILE_FMT}'), 'r') as f:\n",
    "    classes = ast.literal_eval(f.read())\n",
    "# look up the classes from here when needed below\n",
    "print(classes)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "w1bK1QtQiG7d"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "IMAGE_DIR = 'test_images'\n",
    "IMAGE_DIR_PATH = os.path.join(os.getcwd(), IMAGE_DIR)\n",
    "PICTURE_FMT = 'jpg'\n",
    "\n",
    "to_image_path = lambda file_name: os.path.join(IMAGE_DIR_PATH, f'{file_name}.{PICTURE_FMT}')"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "6DQ95qwoiG7e"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1) Occlusion"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "WoWa2Z39iG7e"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### a) Use the provided starter code to generate areas of importance in classification of random images.\n",
    "\n",
    "##### Use the Heatmap to explain qualitatively how well the pre-trained model is able to understand the object of the image.\n",
    "\n",
    "We have given a running example for a basketball image, but continue your exploration for all other images in the directory."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "Ej0_Qym6iG7f"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Starter Code for Creating Occlusions and Generating a Combined HeatMap Image- Note there are better ways of making occlusions by using data augmentation libraries but we want to keep our focus away from those techniques at the moment.\n",
    "\n",
    "loss_function = torch.nn.MSELoss()\n",
    "\n",
    "\n",
    "def generate_area_importance_heatmap_with_occlusions(image, annotation, block_size=14):\n",
    "    image = np.array(image)\n",
    "\n",
    "    height, width, channels = image.shape\n",
    "\n",
    "    columns = width // block_size\n",
    "    rows = height // block_size\n",
    "\n",
    "    heatmap = np.zeros((columns, rows))\n",
    "\n",
    "    for row in range(rows):\n",
    "        for column in range(columns):\n",
    "            x = column * block_size\n",
    "            y = row * block_size\n",
    "\n",
    "            top = int(y)\n",
    "            left = int(x)\n",
    "            right = left + block_size\n",
    "            bottom = top + block_size\n",
    "\n",
    "            tmp_image = np.copy(image)\n",
    "\n",
    "            noise = np.random.rand(block_size, block_size, 3) * 255\n",
    "            tmp_image[int(top):int(bottom), int(left):int(right)] = noise\n",
    "            tmp_image = PIL.Image.fromarray(tmp_image)\n",
    "\n",
    "            preprocessed_image = preprocess_image(tmp_image)\n",
    "            prediction = resnet_model(preprocessed_image).clamp(min=-1, max=1)\n",
    "            loss = round(float(loss_function(prediction, annotation)), 4)\n",
    "\n",
    "            heatmap[column, row] = loss\n",
    "\n",
    "    heatmap = (heatmap - heatmap.min()) / (heatmap.max() - heatmap.min())\n",
    "    heatmap = np.clip(heatmap, 0, 1)\n",
    "    heatmap = heatmap * 255\n",
    "    heatmap = np.uint8(heatmap)\n",
    "    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
    "\n",
    "    heatmap = cv2.resize(heatmap, (width, height), interpolation=cv2.INTER_NEAREST)\n",
    "    heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    overlayed = cv2.addWeighted(image, 1, heatmap, 0.75, 0)\n",
    "    combined = np.concatenate((image, heatmap, overlayed), axis=1)\n",
    "\n",
    "    return combined"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "IP8qak89iG7f"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "PINEAPPLE_TEST_IMAGE_PATH = to_image_path('pineapple')\n",
    "BASKETBALL_TEST_IMAGE_PATH = to_image_path('basketball')\n",
    "SUBMARINE_TEST_IMAGE_PATH = to_image_path('submarine')\n",
    "ZEBRA_TEST_IMAGE_PATH = to_image_path('zebra')\n",
    "SUB_AIRCRAFT_CARRIER_IMAGE_PATH = to_image_path('submarine_and_aircraft_carrier')\n",
    "\n",
    "TEST_IMAGE = PIL.Image.open(BASKETBALL_TEST_IMAGE_PATH)\n",
    "display(TEST_IMAGE)\n",
    "preprocessed_image = preprocess_image(TEST_IMAGE)\n",
    "\n",
    "output = resnet_model(preprocessed_image)\n",
    "occlusion_importance_heatmap = generate_area_importance_heatmap_with_occlusions(np.array(TEST_IMAGE), output)\n",
    "display(PIL.Image.fromarray(occlusion_importance_heatmap))"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "DCNVQCiYiG7g"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Explanation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "65XCqr6HiG7g"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### b) Explain what is important in chosen data set, so that what we as humans intuitively use to identify objects, can be learnt by the machine.\n",
    "\n",
    "Hint: refer back to lecture with example of on mis-classification."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "k7_1zOqTiG7h"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Answer:\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "4jvBQadtiG7h"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2) Unsupervised Explainers\n",
    "\n",
    "Note: This does not require a deep learning model."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "eDFNYZTKiG7i"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load Digits dataset\n",
    "X, y = datasets.load_digits(return_X_y=True)\n",
    "\n",
    "# Split into train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.5, stratify=y, random_state=0\n",
    ")\n",
    "\n",
    "dim = len(X[0])\n",
    "n_classes = len(np.unique(y))"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "DwK2tbDliG7i"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### a) Using PCA find the number of dimensions (axis) with the most variance (i.e. the components have at least 0.1 variance)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "xwrXJ_hriG7j"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pca_results = PCA(n_components=0.95)  # initialise PCA method\n",
    "# TODO: fit PCA\n",
    "print(pca_results.explained_variance_ratio_[:10])  # print the internal calculations"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "4w1LDpoViG7j"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### b) Perform PCA and t-SNE on the Fashion-MNIST dataset using the given the starter code."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "xS5Oupc0iG7j"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Reduce dimension to 2 with PCA\n",
    "pca = make_pipeline(StandardScaler(), PCA(n_components=2, random_state=0))\n",
    "tSNE = ()  #TODO: add method for tSNE with 2 components, random initialisation and perplexity 3 and auto learning rate.\n",
    "\n",
    "dim_reduction_methods = [(\"PCA\", pca), (\"tSNE\", tSNE)]\n",
    "\n",
    "for i, (name, dim_reduction_model) in enumerate(dim_reduction_methods):\n",
    "    plt.figure()\n",
    "\n",
    "    # TODO: Fit the method's model\n",
    "\n",
    "    # Embed the data set in 2 dimensions using the fitted model\n",
    "    if name == \"PCA\":\n",
    "        X_embedded = dim_reduction_model.transform(X)\n",
    "    else:\n",
    "        X_embedded = dim_reduction_model.fit_transform(X)\n",
    "\n",
    "    # Plot the projected points and show the evaluation score\n",
    "    plt.scatter(X_embedded[:, 0], X_embedded[:, 1], c=y, s=30, cmap=\"Set1\")\n",
    "    plt.title(f\"{name}\")\n",
    "plt.show()"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "QpzBN2NfiG7k"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### c) Explain what can be interpreted using an unsupervised method like this in explaining an incorrect classification."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "WeCEmNnKiG7l"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Answer:\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "O_nrIMKSiG7t"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3) Gradient Based Explainers"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "FJXdIa9QiG7u"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from utils import preprocess_image, overlay_heatmap_on_image\n",
    "from gradcam import GradCam"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "rQrWjHEPiG7u"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### a) Use the given data set and pretrained model to generate heatmap overlay for which part of the image explains the classification, i.e. perform a CAM for a random image.\n",
    "\n",
    "A running example of aircraft carrier and submarine is given"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "Qlxc6RoEiG7u"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "TEST_IMAGE = PIL.Image.open(SUB_AIRCRAFT_CARRIER_IMAGE_PATH)\n",
    "display(TEST_IMAGE)\n",
    "preprocessed_image = preprocess_image(TEST_IMAGE)\n",
    "\n",
    "output = resnet_model(preprocessed_image)\n",
    "softmax = torch.softmax(output, dim=1)\n",
    "\n",
    "top2 = torch.topk(softmax, 2).indices.numpy()\n",
    "\n",
    "for idx, class_idx in enumerate(top2[0]):\n",
    "    print(idx, f'imagenet_idx={class_idx}', classes[class_idx])"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "9d98wt_yiG7w"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gradcam = GradCam(model=resnet_model, target_layer=resnet_model.layer4, target_layer_names=[\"1\"])\n",
    "\n",
    "def generate_cam_overlay_for_target_class(processed_image, target_class=None):\n",
    "    img = np.array(TEST_IMAGE)\n",
    "    img = img[:, :, ::-1].copy()\n",
    "    img = np.float32(img) / 255\n",
    "\n",
    "    cam_heatmap = gradcam(processed_image, target_class)\n",
    "    cam_overlay = overlay_heatmap_on_image(img, cam_heatmap)\n",
    "    return cam_overlay\n",
    "\n",
    "submarine_cam_overlay = generate_cam_overlay_for_target_class(preprocessed_image, 833)\n",
    "display(PIL.Image.fromarray(cv2.cvtColor(submarine_cam_overlay, cv2.COLOR_BGR2RGB)))"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "ArZAaqCniG7w"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### b) Explain how when there are multiple objects in a frame, this could be used identify each object individually."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "BaXy2tndiG7w"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Explanation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "tff8LDjOiG7w"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4) DeepSHAP (DeepLift)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "7XhfMUykiG7x"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# imports for a deep learning model\n",
    "import torch, torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import numpy as np"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "CaCh1TyWiG7x"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### a) Use the shap library and the provided starter code to explain the MNIST data set."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "GHkANYpOiG7x"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# simple code for training a deep learning model with pyTorch\n",
    "device = torch.device('cpu')\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(1, 10, kernel_size=5),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(10, 20, kernel_size=5),\n",
    "            nn.Dropout(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(320, 50),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(50, 10),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.view(-1, 320)\n",
    "        x = self.fc_layers(x)\n",
    "        return x\n",
    "\n",
    "def train(model_to_train, train_on, train_data_loader, optimizer_func, epoch_num):\n",
    "    model_to_train.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_data_loader):\n",
    "        data, target = data.to(train_on), target.to(train_on)\n",
    "        optimizer_func.zero_grad()\n",
    "        output = model_to_train(data)\n",
    "        loss = F.nll_loss(output.log(), target)\n",
    "        loss.backward()\n",
    "        optimizer_func.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch_num, batch_idx * len(data), len(train_data_loader.dataset),\n",
    "                           100. * batch_idx / len(train_data_loader), loss.item()))\n",
    "\n",
    "def test(model_to_test, train_on, test_data_loader):\n",
    "    model_to_test.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_data_loader:\n",
    "            data, target = data.to(train_on), target.to(train_on)\n",
    "            output = model_to_test(data)\n",
    "            test_loss += F.nll_loss(output.log(), target).item() # sum up batch loss\n",
    "            pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_data_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_data_loader.dataset),\n",
    "    100. * correct / len(test_data_loader.dataset)))"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "Q7LGPQO4iG7y"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "resnet_model = Net().to(device)\n",
    "optimizer = optim.SGD(resnet_model.parameters(), lr=0.01, momentum=0.5)\n",
    "\n",
    "batch_size = 128\n",
    "num_epochs = 2\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(\n",
    "        'mnist_data',\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=transforms.Compose([transforms.ToTensor()])\n",
    "    ),\n",
    "    batch_size=batch_size, shuffle=True\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(\n",
    "        'mnist_data',\n",
    "        train=False,\n",
    "        transform=transforms.Compose([transforms.ToTensor()])\n",
    "    ),\n",
    "    batch_size=batch_size, shuffle=True\n",
    ")\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train(resnet_model, device, train_loader, optimizer, epoch)\n",
    "    test(resnet_model, device, test_loader)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "5fvTB6z4iG7z"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "batch = next(iter(test_loader))\n",
    "images, _ = batch\n",
    "\n",
    "background = images[:100]\n",
    "test_images = images[100:103]\n",
    "\n",
    "explainer = ()  # TODO: use the DeepExplainer on the model and background\n",
    "shap_values = ()  # TODO: find the shap values using the explainer for the test_images\n",
    "\n",
    "shap_numpy = [np.swapaxes(np.swapaxes(s, 1, -1), 1, 2) for s in shap_values]\n",
    "test_numpy = np.swapaxes(np.swapaxes(test_images.numpy(), 1, -1), 1, 2)\n",
    "\n",
    "shap.image_plot(shap_numpy, -test_numpy)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "7CSdGo-IiG7z"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### b) Explain what can be understood about the similarity of certain digits as seen by the machine learning model through the explanations."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "UFbDDKWBiG70"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Answer:\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "-NtUVOW8iG70"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5) Attacks and Defence using Adversarial Features"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "UbpO5tb1iG71"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### a) Read the article [NeurIPS 2017 - Adversarial Attacks and Defences Competition](https://arxiv.org/pdf/1804.00097.pdf), mainly Section 2.2 and 2.3 and explain the general idea what the attack and defence try to achieve. (Explain at least 2 different of each attack and defence)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "owhAqt_SiG71"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Answer:\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "I_A5JDXwiG71"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "WI39y8roiG72"
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
